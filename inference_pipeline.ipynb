{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Slip and Fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__version__ = 1.5\n",
    "__author__ = 'Ajay Bhargava'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import VideoReaders, DetectorLoader\n",
    "import numpy as np, pandas as pd, glob\n",
    "import cv2\n",
    "from skimage import measure, color, segmentation\n",
    "from SORT.sort import *\n",
    "import warnings\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainedAssignment:\n",
    "    def __init__(self, chained=None):\n",
    "        acceptable = [None, 'warn', 'raise']\n",
    "        assert chained in acceptable, \"chained must be in \" + str(acceptable)\n",
    "        self.swcw = chained\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.saved_swcw = pd.options.mode.chained_assignment\n",
    "        pd.options.mode.chained_assignment = self.swcw\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        pd.options.mode.chained_assignment = self.saved_swcw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fall(path, video_output = False):\n",
    "    '''\n",
    "    Docstring for detect_slip_fall()\n",
    "\n",
    "    Arguments\n",
    "    -----------\n",
    "    str(path): Takes a path and sends it to the function for inference and subsequent action\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    output(dict): A dictionary with the output of the model {\"Frame\": frame, \"BBOX\": bbox} \n",
    "    which is the first frame and person position where the person fell down. \n",
    "    '''\n",
    "\n",
    "    frame_provider = VideoReaders.VideoReader(path)\n",
    "    length, shape = frame_provider.properties()\n",
    "\n",
    "    output_inference = []\n",
    "    frames = []\n",
    "\n",
    "    # Run Model\n",
    "    for frame in frame_provider:\n",
    "        frames.append(frame)\n",
    "        c, s, bb, ma = model.predict(frame)\n",
    "        idx = np.where(c == 0)\n",
    "        pixelwise_arrays = []\n",
    "        for item in idx:\n",
    "            for n, qq in enumerate(item):\n",
    "                pixelwise = np.zeros_like(ma[qq,...])\n",
    "                pixelwise = ma[qq,...].astype(np.uint8)\n",
    "                pixelwise[np.where(pixelwise == 1)] = n + 1\n",
    "                pixelwise_arrays.append(pixelwise)\n",
    "\n",
    "        filtered = []\n",
    "        for array in pixelwise_arrays:\n",
    "            if np.max(array) != 0:\n",
    "                filtered.append(array)\n",
    "\n",
    "        stacked = np.sum(filtered, axis = 0)\n",
    "        output_inference.append(stacked)\n",
    "\n",
    "    output = []\n",
    "    for arry in output_inference:\n",
    "        if len(arry.shape) < 2:\n",
    "            output.append(np.zeros(shape).astype(np.uint8))\n",
    "        else:\n",
    "            output.append(arry)\n",
    "    \n",
    "    if len(output) != len(frames):\n",
    "        raise ValueError(\"Frame Lengths are off.\")\n",
    "\n",
    "    if video_output:\n",
    "        boundaries = []\n",
    "        for im, mask in zip(frames, output):\n",
    "            boundaries.append(segmentation.mark_boundaries(im, mask, mode = 'thick', color = (1,0,0)))\n",
    "\n",
    "    # Derive measurement of angles\n",
    "    minimal_dictionary = []\n",
    "    for i in range(0, length):\n",
    "        for region in measure.regionprops(output[i], color.rgb2gray(frames[i])):\n",
    "            minimal_dictionary.append({\n",
    "                \"Frame\": i, \n",
    "                \"ID\": region.label, \n",
    "                \"BBOX\": region.bbox\n",
    "            })\n",
    "    tracked_bboxes = []\n",
    "    Sorter = Sort(max_age = length, min_hits = 1, iou_threshold = 0.01) # 1 Tunable parameter (iou_threshold)\n",
    "    for instance, entry in groupby(minimal_dictionary, key = lambda x:x['Frame']):\n",
    "        entry_list = []\n",
    "        for item in entry:\n",
    "            lst = list(item['BBOX'])\n",
    "            lst.extend([0, item['ID']])\n",
    "            entry_list.append(lst)\n",
    "        track_bbs_ids = Sorter.update(np.array(entry_list))\n",
    "        for objects in track_bbs_ids:\n",
    "            r0, c0, r1, c1, ID, label = objects.tolist()\n",
    "            for region in measure.regionprops(output[instance], color.rgb2gray(frames[instance])):\n",
    "                if region.label == label:\n",
    "                    this_normalized_moment = region.moments_normalized\n",
    "                    angle = np.degrees((np.arctan2(2*this_normalized_moment[1,1], this_normalized_moment[2,0] - this_normalized_moment[0,2]))/2)\n",
    "                    w = region.bbox[2] - region.bbox[0]\n",
    "                    h = region.bbox[3] - region.bbox[1]\n",
    "                    ar = w / float(h)\n",
    "                    tracked_bboxes.append({'Frame': instance, \n",
    "                                            'ID': int(label), \n",
    "                                            'Track ID': int(ID), \n",
    "                                            'BBOX': region.bbox,\n",
    "                                            'Angle': angle,\n",
    "                                            'Area': region.area,\n",
    "                                            'Aspect Ratio': ar, \n",
    "                                            'Eccentricity': region.eccentricity, \n",
    "                                            'Perimeter': region.perimeter})\n",
    "    tracked_dataframe = pd.DataFrame(tracked_bboxes)\n",
    "    # Note frames where fall was detected\n",
    "    super_output = []\n",
    "    if len(tracked_dataframe) == 0:\n",
    "        super_output.append({'File': None, 'Frame': None, 'Bounding Box': None})\n",
    "    else:\n",
    "        for tracks, track_df in tracked_dataframe.groupby('Track ID'):\n",
    "            current_track = track_df.loc[(track_df['Track ID'] == tracks), :]\n",
    "            with ChainedAssignment():\n",
    "                current_track['Rate of Change'] = current_track.loc[(current_track['Track ID'] == tracks), :]['Angle'].pct_change(5, fill_method = 'ffill') # 1 Tunable parameters (Periodicity for rate of change in angle)\n",
    "                current_track['Slip-Fall'] = np.where(current_track['Rate of Change'] < -10, 'Slip', 'Stand') # 1 Tunable Parameter (Hard Cutoff for % change in angle)\n",
    "            if len(current_track.loc[current_track['Slip-Fall'] == 'Slip']) != 0:\n",
    "                super_output.append({'Track ID': tracks, 'File': os.path.basename(path), 'Frame': current_track.loc[current_track['Slip-Fall'] == 'Slip'].iloc[0]['Frame'], 'Bounding Box': current_track.loc[current_track['Slip-Fall'] == 'Slip'].iloc[0]['BBOX']})\n",
    "            else:\n",
    "                super_output.append({'Track ID': None, 'File': None, 'Frame': None, 'Bounding Box': None})\n",
    "    \n",
    "    fall_ids = pd.DataFrame(super_output)\n",
    "    fall_ids = fall_ids.dropna()\n",
    "\n",
    "\n",
    "    if video_output:\n",
    "        aggregate_bbox_df = []\n",
    "        \n",
    "        if len(fall_ids) == 0:\n",
    "            return boundaries, fall_ids\n",
    "        else:\n",
    "            for track, data in fall_ids.groupby('Track ID'):\n",
    "                investigate_frame = int(data['Frame'].values)\n",
    "                investigate_track = track\n",
    "                cv2_dataframe = tracked_dataframe[(tracked_dataframe['Track ID'] == investigate_track) & (tracked_dataframe['Frame'] >= investigate_frame)][['Track ID', 'Frame', 'BBOX', 'Angle']]\n",
    "                aggregate_bbox_df.append(cv2_dataframe)\n",
    "            \n",
    "            if len(aggregate_bbox_df) == 0:\n",
    "                return boundaries, fall_ids\n",
    "            else:\n",
    "                concat_bbox_df = pd.concat(aggregate_bbox_df)\n",
    "                bbox_dictionary = concat_bbox_df.groupby('Frame').agg(tuple).applymap(list).reset_index()\n",
    "                cv2_final_dictionary = pd.concat([bbox_dictionary.set_index('Frame').reindex(range(0, bbox_dictionary.Frame.min())).ffill().reset_index(), bbox_dictionary])\n",
    "                \n",
    "                final_frames = []\n",
    "                for frame, info in cv2_final_dictionary.groupby('Frame'):\n",
    "                    if info.loc[info['Frame'] == frame]['Track ID'].isnull().values:\n",
    "                        final_frames.append(boundaries[frame])\n",
    "                    else:\n",
    "                        for angle, item in zip(info['Angle'], info['BBOX']):\n",
    "                            for t, tuple_object in zip(angle, item):\n",
    "                                t = str(t)[:4] + 'Degrees'\n",
    "                                x = tuple_object[0]\n",
    "                                y = tuple_object[1]\n",
    "                                w = tuple_object[2] - tuple_object[0]\n",
    "                                h = tuple_object[3] - tuple_object[1]\n",
    "                                image_output = cv2.rectangle(boundaries[frame], (y,x), (y + h, x + w), (36,255,12), 1)\n",
    "                                image_output = cv2.putText(image_output, str(t), (y, x - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "                        final_frames.append(image_output)\n",
    "                return final_frames, fall_ids\n",
    "    else:\n",
    "        return fall_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_list = [0.1, 0.15, 0.20, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "pathlist = sorted(glob.glob('../datasets/slip-fall/fall-database/*.mp4'))\n",
    "for confidence_value in confidence_list:\n",
    "    for path in pathlist[25:26]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model = DetectorLoader.YOLACT('./weights/yolact_resnet50_54_800000.pth', threshold = confidence_value)\n",
    "        path_out = os.path.basename(path)\n",
    "        final_frames, fall_ids = detect_fall(path, video_output = True)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        converted_confidence_value = str(confidence_value).replace('.', '-')\n",
    "        if not os.path.exists(\"./results/slip-videos/{}-confidence-threshold\".format(converted_confidence_value)):\n",
    "            os.makedirs(\"./results/slip-videos/{}-confidence-threshold\".format(converted_confidence_value))\n",
    "        writer = cv2.VideoWriter(\"./results/slip-videos/{}-confidence-threshold/{}\".format(converted_confidence_value, path_out), 0x7634706d, 5.0, max([x.shape for x in final_frames])[:-1][::-1])\n",
    "        for capture in final_frames:\n",
    "            out_capture = (capture * 255).astype(np.uint8)\n",
    "            writer.write(out_capture)\n",
    "        writer.release()\n",
    "        if not os.path.exists(\"./results/charts/slip-videos/{}-confidence-threshold\"):\n",
    "            os.makedirs(\"./results/charts/slip-videos/{}-confidence-threshold\".format(converted_confidence_value))\n",
    "        fall_ids.to_csv('./results/charts/slip-videos/{}-confidence-threshold/{}.csv'.format(converted_confidence_value, os.path.splitext(path_out)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_list = [0.1]\n",
    "pathlist = sorted(glob.glob('../datasets/slip-fall/no-fall-database/*.mp4'))\n",
    "for confidence_value in confidence_list:\n",
    "    for path in pathlist[253:254]:\n",
    "        print(path)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model = DetectorLoader.YOLACT('./weights/yolact_resnet50_54_800000.pth', threshold = confidence_value)\n",
    "        path_out = os.path.basename(path)\n",
    "        final_frames, fall_ids = detect_fall(path, video_output = True)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        converted_confidence_value = str(confidence_value).replace('.', '-')\n",
    "        if not os.path.exists(\"./results/no-slip-videos/{}-confidence-threshold/\".format(converted_confidence_value)):\n",
    "            os.makedirs(\"./results/no-slip-videos/{}-confidence-threshold/\".format(converted_confidence_value))\n",
    "        writer = cv2.VideoWriter(\"./results/no-slip-videos/{}-confidence-threshold/{}\".format(converted_confidence_value, path_out), 0x7634706d, 5.0, max([x.shape for x in final_frames])[:-1][::-1])\n",
    "        for capture in final_frames:\n",
    "            out_capture = (capture * 255).astype(np.uint8)\n",
    "            writer.write(out_capture)\n",
    "        writer.release()\n",
    "        if not os.path.exists(\"./results/charts/no-slip-videos/{}-confidence-threshold/\".format(converted_confidence_value)):\n",
    "            os.makedirs(\"./results/charts/no-slip-videos/{}-confidence-threshold/\".format(converted_confidence_value))\n",
    "        fall_ids.to_csv('./results/charts/no-slip-videos/{}-confidence-threshold/{}.csv'.format(converted_confidence_value, os.path.splitext(path_out)[0]))\n",
    "\n",
    "        vars = ['final_frames', 'fall_ids', 'model']\n",
    "        for v in vars:\n",
    "            del v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7997ca2cf6f82a32cba4455fba40e779da088b6fcef0f47d94689f04f3d2f0e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('YOLACT': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
