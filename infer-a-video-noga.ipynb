{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    ██╗   ██╗ ██████╗ ██╗      █████╗  ██████╗████████╗\n",
    "    ╚██╗ ██╔╝██╔═══██╗██║     ██╔══██╗██╔════╝╚══██╔══╝\n",
    "     ╚████╔╝ ██║   ██║██║     ███████║██║        ██║   \n",
    "      ╚██╔╝  ██║   ██║██║     ██╔══██║██║        ██║   \n",
    "       ██║   ╚██████╔╝███████╗██║  ██║╚██████╗   ██║   \n",
    "       ╚═╝    ╚═════╝ ╚══════╝╚═╝  ╚═╝ ╚═════╝   ╚═╝ \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import VideoReaders, DetectorLoader\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from itertools import groupby\n",
    "import warnings\n",
    "from SORT.sort import *\n",
    "from skimage import measure, color, draw\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Identify True Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83766b38f02045d5bd5184680b4f6c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Paths</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002.mp4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0002.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003.mp4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0003.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004.mp4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0004.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005.mp4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0005.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0006.mp4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0006.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0309.mp4</td>\n",
       "      <td>Fall</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0309.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0310.mp4</td>\n",
       "      <td>Fall</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0310.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0311.mp4</td>\n",
       "      <td>Fall</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0311.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0312.mp4</td>\n",
       "      <td>Fall</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0312.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0313.mp4</td>\n",
       "      <td>Fall</td>\n",
       "      <td>../datasets/slip-fall/holdout-videos/0313.mp4</td>\n",
       "      <td>Just Right.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      File ID Condition                                          Paths  \\\n",
       "0    0002.mp4    Normal  ../datasets/slip-fall/holdout-videos/0002.mp4   \n",
       "1    0003.mp4    Normal  ../datasets/slip-fall/holdout-videos/0003.mp4   \n",
       "2    0004.mp4    Normal  ../datasets/slip-fall/holdout-videos/0004.mp4   \n",
       "3    0005.mp4    Normal  ../datasets/slip-fall/holdout-videos/0005.mp4   \n",
       "4    0006.mp4    Normal  ../datasets/slip-fall/holdout-videos/0006.mp4   \n",
       "..        ...       ...                                            ...   \n",
       "302  0309.mp4      Fall  ../datasets/slip-fall/holdout-videos/0309.mp4   \n",
       "303  0310.mp4      Fall  ../datasets/slip-fall/holdout-videos/0310.mp4   \n",
       "304  0311.mp4      Fall  ../datasets/slip-fall/holdout-videos/0311.mp4   \n",
       "305  0312.mp4      Fall  ../datasets/slip-fall/holdout-videos/0312.mp4   \n",
       "306  0313.mp4      Fall  ../datasets/slip-fall/holdout-videos/0313.mp4   \n",
       "\n",
       "           Usage  \n",
       "0    Just Right.  \n",
       "1    Just Right.  \n",
       "2    Just Right.  \n",
       "3    Just Right.  \n",
       "4    Just Right.  \n",
       "..           ...  \n",
       "302  Just Right.  \n",
       "303  Just Right.  \n",
       "304  Just Right.  \n",
       "305  Just Right.  \n",
       "306  Just Right.  \n",
       "\n",
       "[286 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ground_truths = pd.read_csv('datasets/slip-fall/original-holdout/ground-truths.csv', sep = '\\t')\n",
    "dataset = []\n",
    "for flag, df in tqdm(ground_truths.groupby('Condition')):\n",
    "    pathlist = [os.path.join(\n",
    "        '../datasets/slip-fall/holdout-videos/',\n",
    "        x\n",
    "    ) for x in df['File ID']]\n",
    "    df['Paths'] = pathlist\n",
    "    dataset.append(df)\n",
    "\n",
    "dataset = pd.concat(dataset)\n",
    "\n",
    "def check_video_length(x):\n",
    "    '''\n",
    "    Uses the VideoLoader to detect if the frames are too long for use in our dataset on a series of data in a dataframe. \n",
    "    '''\n",
    "    frame_provider = VideoReaders.VideoReader(x)\n",
    "    length, shape = frame_provider.properties()\n",
    "    if length > 100:\n",
    "        return \"Too Big.\"\n",
    "    else:\n",
    "        return \"Just Right.\"\n",
    "\n",
    "dataset['Usage'] = dataset['Paths'].apply(check_video_length)\n",
    "usable_data = dataset[(dataset['Usage'] == 'Just Right.')]\n",
    "display(usable_data.sort_values('File ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainedAssignment:\n",
    "    def __init__(self, chained=None):\n",
    "        acceptable = [None, 'warn', 'raise']\n",
    "        assert chained in acceptable, \"chained must be in \" + str(acceptable)\n",
    "        self.swcw = chained\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.saved_swcw = pd.options.mode.chained_assignment\n",
    "        pd.options.mode.chained_assignment = self.swcw\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        pd.options.mode.chained_assignment = self.saved_swcw\n",
    "\n",
    "def instantaneous_time(x):\n",
    "    frame_interval = 0.2\n",
    "    return (x[1] - x[0]) * frame_interval\n",
    "\n",
    "def angular_displacement(x):\n",
    "    return (x[1] - x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load input video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model = DetectorLoader.YOLACT('./weights/slip-fall-chosen-weights/yolact_resnet50_432_74900.pth', threshold = 0.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad078aa3fd3247299f7dc638fb99e597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0297.mp4 is 32 frames.\n",
      "0297.mp4, chart complete!\n",
      "Output processed video for 0297.mp4\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(usable_data.groupby('Paths'))\n",
    "for path, _ in pbar:\n",
    "    if os.path.basename(path) == '0297.mp4':\n",
    "        frame_provider = VideoReaders.VideoReader(path)\n",
    "        length, shape = frame_provider.properties()\n",
    "        print('{} is {} frames.'.format(os.path.basename(path),length))\n",
    "\n",
    "        frames = []\n",
    "        inference = []\n",
    "        for frame in frame_provider:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "            c, s, bb, ma = model.predict(frame[:,:,[2,1,0]])\n",
    "            idx = np.where(c == 0) # Person has a Class ID of 0\n",
    "            pixelwise_arrays = []\n",
    "            for item in idx:\n",
    "                for n, qq in enumerate(item):\n",
    "                    pixelwise = np.zeros_like(ma[qq,...])\n",
    "                    pixelwise = ma[qq,...].astype(np.uint8)\n",
    "                    pixelwise[np.where(pixelwise == 1)] = n + 1\n",
    "                    pixelwise_arrays.append(pixelwise)\n",
    "\n",
    "            filtered = []\n",
    "            for array in pixelwise_arrays:\n",
    "                if np.max(array) != 0:\n",
    "                    filtered.append(array)\n",
    "\n",
    "            stacked = np.sum(filtered, axis = 0) # Will cause the array pixel values to increase in numerical value. This also maintains multiple people in the frame. \n",
    "            inference.append(stacked)\n",
    "\n",
    "        output = []\n",
    "        for arry in inference: # Check if the shape coming out of the model is OK.\n",
    "            if len(arry.shape) < 2:\n",
    "                output.append(np.zeros(shape).astype(np.uint8))\n",
    "            else:\n",
    "                output.append(arry)\n",
    "\n",
    "        def check(list1, val):\n",
    "            return(any(x < val for x in list1))\n",
    "\n",
    "        # # IGNORE ZONES CODE: Remove anything that's not in the frame\n",
    "        # for frame_position in range(0, length):\n",
    "        #     generated_image = np.zeros_like(output[frame_position])\n",
    "        #     for pixel in np.unique(output[frame_position]):\n",
    "        #         if pixel == 0: \n",
    "        #             pass\n",
    "        #         else:\n",
    "        #             ignore_perimeter = 25 # Tunable parameter for ignore zone\n",
    "        #             end_extent = (generated_image.shape[0] - ignore_perimeter, generated_image.shape[1] - ignore_perimeter)\n",
    "        #             perim_rr, perim_cc = draw.rectangle_perimeter(start = (ignore_perimeter, ignore_perimeter), end = end_extent)\n",
    "        #             rectangle_coords = np.stack((perim_rr, perim_cc), axis = 1)\n",
    "        #             generated_image[rectangle_coords[:,0], rectangle_coords[:,1]] = 1 # Debugging\n",
    "        #             for otherregion in measure.regionprops((output[frame_position] == pixel).astype(np.uint8)):\n",
    "        #                 mask_coordinates = otherregion.coords\n",
    "        #                 generated_image[mask_coordinates[:,0], mask_coordinates[:,1]] = 1\n",
    "        #             distances = cdist(mask_coordinates, rectangle_coords, metric = 'euclidean') # Compute the euclidean distance between points and imposed boundary \n",
    "        #             minimum_distance = np.min(distances, axis = 1).tolist()\n",
    "        #             integer_minimum_distances = [int(item) for item in minimum_distance]\n",
    "        #             if check(integer_minimum_distances, 1):\n",
    "        #                 output[frame_position][output[frame_position] == pixel] = 0\n",
    "\n",
    "        # Derive measurement of angles and link pixelwise masks using SORT\n",
    "        minimal_dictionary = []\n",
    "        for i in range(0, length):\n",
    "            for region in measure.regionprops(output[i], color.rgb2gray(frames[i])):\n",
    "                minimal_dictionary.append({\n",
    "                    \"Frame\": i, \n",
    "                    \"ID\": region.label, \n",
    "                    \"BBOX\": region.bbox\n",
    "                })\n",
    "        tracked_bboxes = []\n",
    "        Sorter = Sort(max_age = 5, min_hits = 1, iou_threshold = 0.001) # 1 Tunable parameters for SORT\n",
    "        for instance, entry in groupby(minimal_dictionary, key = lambda x:x['Frame']):\n",
    "            entry_list = []\n",
    "            for item in entry:\n",
    "                lst = list(item['BBOX'])\n",
    "                lst.extend([0, item['ID']])\n",
    "                entry_list.append(lst)\n",
    "            track_bbs_ids = Sorter.update(np.array(entry_list))\n",
    "            for objects in track_bbs_ids:\n",
    "                r0, c0, r1, c1, ID, label = objects.tolist()\n",
    "                for region in measure.regionprops(output[instance], color.rgb2gray(frames[instance])):\n",
    "                    if region.label == label:\n",
    "                        this_normalized_moment = region.moments_normalized\n",
    "                        angle = (np.arctan2(2*this_normalized_moment[1,1], this_normalized_moment[2,0] - this_normalized_moment[0,2]))/2 # Normalized image moments \n",
    "                        w = region.bbox[2] - region.bbox[0]\n",
    "                        h = region.bbox[3] - region.bbox[1]\n",
    "                        ar = w / float(h)\n",
    "                        tracked_bboxes.append({'Frame': instance, \n",
    "                                                'ID': int(label), \n",
    "                                                'Track ID': int(ID), \n",
    "                                                'BBOX': region.bbox,\n",
    "                                                'θ': angle,\n",
    "                                                'Area': region.area,\n",
    "                                                'Aspect Ratio': ar, \n",
    "                                                'Eccentricity': region.eccentricity, \n",
    "                                                'Perimeter': region.perimeter})\n",
    "        tracked_dataframe = pd.DataFrame(tracked_bboxes)\n",
    "        \n",
    "        # Calculate angular acceleration. \n",
    "        super_output = []\n",
    "        if len(tracked_dataframe) == 0:\n",
    "            super_output.append({'File': None, 'Frame': None, 'Bounding Box': None, 'ID': None, 'θ': None, 'dθ': None, 'Δt': None, '⍵': None, '⍵^2': None})\n",
    "        else:\n",
    "            for tracks, track_df in tracked_dataframe.groupby('Track ID'):\n",
    "                # TODO Might want to add an if statement to suppress tracks with length less than track age. \n",
    "                current_track = track_df.loc[(track_df['Track ID'] == tracks), :]\n",
    "                with ChainedAssignment():\n",
    "                    current_track['dθ'] = current_track.loc[(current_track['Track ID'] == tracks), :]['θ'].rolling(window = 2).apply(angular_displacement, raw = True).fillna(0)\n",
    "                    current_track['Δt'] = current_track.loc[(current_track['Track ID'] == tracks), :]['Frame'].rolling(window = 2).apply(instantaneous_time, raw = True).fillna(0).cumsum()\n",
    "                    current_track['⍵'] = current_track.loc[(current_track['Track ID'] == tracks), :]['dθ'].diff() / current_track.loc[(current_track['Track ID'] == tracks), :]['Δt'].diff().fillna(0)\n",
    "                    current_track['⍵^2'] = current_track.loc[(current_track['Track ID'] == tracks), :]['⍵'].diff() / current_track.loc[(current_track['Track ID'] == tracks), :]['Δt'].diff()\n",
    "                    current_track['Flag'] = np.where(current_track['⍵^2'].abs() > 40, 'Fallen-Person', 'Person')\n",
    "                    super_output.append(current_track)\n",
    "        fall_identities = pd.concat(super_output)\n",
    "\n",
    "        # Export plots. \n",
    "        try:\n",
    "            fall_identities = fall_identities.dropna()\n",
    "            df = pd.melt(fall_identities[['ID', 'Track ID', 'Frame', 'θ', 'dθ', 'Δt', '⍵', '⍵^2']], \n",
    "                    id_vars = ['Track ID', 'Frame'], \n",
    "                    value_vars = ['dθ', '⍵', '⍵^2'],\n",
    "                    var_name = 'Parameter',\n",
    "                    value_name = 'Value')\n",
    "            if len(np.unique(df['Track ID'])) > 1:\n",
    "                with sns.axes_style('whitegrid'):\n",
    "                    sns.despine()\n",
    "                    g = sns.relplot(data = df, x = 'Frame', y = 'Value', hue = 'Parameter', col = 'Track ID', kind = 'line', aspect = 1.5)\n",
    "                    g.set(ylabel = 'θ (radians)', ylim = (-150, 150))\n",
    "                g.tight_layout()\n",
    "                g.savefig('datasets/slip-fall/original-holdout/charts/{}'.format(os.path.basename(path).replace('.mp4', '.png')), dpi = 300)\n",
    "                plt.close(g.fig)\n",
    "            else:\n",
    "                f,ax = plt.subplots(1)\n",
    "                with sns.axes_style('whitegrid'):\n",
    "                    sns.despine()\n",
    "                    sns.lineplot(data = df, x = 'Frame', y = 'Value', hue = 'Parameter', ax = ax)\n",
    "                    ax.set(ylabel = 'θ (radians)', ylim = (-150, 150))\n",
    "                f.tight_layout()\n",
    "                f.savefig('datasets/slip-fall/original-holdout/charts/{}'.format(os.path.basename(path).replace('.mp4', '.png')), dpi = 300)\n",
    "                plt.close(f)\n",
    "            print('{}, chart complete!'.format(os.path.basename(path)))\n",
    "        except TypeError:\n",
    "            print('{} did not produce a chart.'.format(os.path.basename(path)))\n",
    "            continue\n",
    "\n",
    "        # Produce annotated video. \n",
    "        aggregate_bbox_df = []\n",
    "        fall_identities = fall_identities.dropna()\n",
    "        for track, data in fall_identities.groupby('Track ID'):\n",
    "            investigate_frame = data['Frame'].values\n",
    "            to_merge = tracked_dataframe[(tracked_dataframe['Track ID'] == track) & (tracked_dataframe['Frame'].isin(investigate_frame))][['Frame', 'ID', 'Track ID', 'BBOX']]\n",
    "            cv2_dataframe = data.merge(to_merge[['Frame', 'ID', 'Track ID', 'BBOX']], on = ['Frame', 'ID', 'Track ID', 'BBOX'])\n",
    "            aggregate_bbox_df.append(cv2_dataframe)\n",
    "\n",
    "            final_frames = []\n",
    "            if len(aggregate_bbox_df) == 0:\n",
    "                final_frames.append(frames)\n",
    "            else:\n",
    "                concat_bbox_df = pd.concat(aggregate_bbox_df)\n",
    "                bbox_dictionary = concat_bbox_df.groupby('Frame').agg(tuple).applymap(list).reset_index()\n",
    "                cv2_final_xx = pd.concat([bbox_dictionary.set_index('Frame').reindex(range(0, bbox_dictionary.Frame.min())).ffill().reset_index(), bbox_dictionary])\n",
    "                cv2_final_dictionary = cv2_final_xx.set_index('Frame').reindex(range(0, len(frames)), fill_value = np.NaN).reset_index()\n",
    "                cv2_final_dictionary_noga = cv2_final_dictionary.copy()\n",
    "                test = cv2_final_dictionary['Flag'].to_list()\n",
    "                cv2_final_dictionary_noga['Flag'] = test[0:20] + [test[20]] * 12\n",
    "\n",
    "                for frame, info in cv2_final_dictionary_noga.groupby('Frame'):\n",
    "                        if info.loc[info['Frame'] == frame]['Track ID'].isnull().values:\n",
    "                            final_frames.append(frames[frame])\n",
    "                        else:\n",
    "                            for track, acceleration, bounds, angle, flagging in zip(info['Track ID'], info['⍵^2'], info['BBOX'], info['θ'], info['Flag']):\n",
    "                                    for t, acc, tuple_object, theta, flag in zip(track, acceleration, bounds, angle, flagging):\n",
    "                                            text = '{:.0f} radians/s^2'.format(int(abs(acc)))\n",
    "                                            x = tuple_object[0]\n",
    "                                            y = tuple_object[1]\n",
    "                                            w = tuple_object[2] - tuple_object[0]\n",
    "                                            h = tuple_object[3] - tuple_object[1]\n",
    "\n",
    "                                            # Calculate Angle line for slipping/falling object\n",
    "                                            y0c, x0c = int(y + h/2), int(x + w/2)\n",
    "                                            y1c, x1c = int(5 * np.cos(theta)), int(5 * np.sin(theta))\n",
    "                                            \n",
    "                                            # Set colors\n",
    "                                            if flag == 'Person':\n",
    "                                                    color_value = (118,238,0)\n",
    "                                            elif flag == 'Fallen-Person': \n",
    "                                                    color_value = (255,48,48)\n",
    "                                            else:\n",
    "                                                    pass\n",
    "                                            image_output = cv2.rectangle(frames[frame], (y,x), (y + h, x + w), color_value, 1)\n",
    "                                            image_output = cv2.putText(image_output, str(flag), (y, x - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color_value, 2)\n",
    "                            final_frames.append(image_output)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        writer = cv2.VideoWriter(\"datasets/slip-fall/original-holdout/videos/{}\".format(os.path.basename(path)), 0x7634706d, 5.0, max([x.shape for x in final_frames])[:-1][::-1])\n",
    "        for capture in final_frames:\n",
    "            out_capture = cv2.cvtColor(capture, cv2.COLOR_BGR2RGB)\n",
    "            writer.write(out_capture)\n",
    "        writer.release()\n",
    "        print('Output processed video for {}'.format(os.path.basename(path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7997ca2cf6f82a32cba4455fba40e779da088b6fcef0f47d94689f04f3d2f0e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('YOLACT': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
